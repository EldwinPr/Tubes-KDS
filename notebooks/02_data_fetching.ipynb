{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04137d4a",
   "metadata": {},
   "source": [
    "Setup & Import Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "574fa58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 13:04:38,762 - INFO - Starting data fetching notebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PREVIOUS EXPLORATION RESULTS:\n",
      "Total sequences available: 327\n",
      "Target species: Biston betularia\n",
      "Exploration date: 2025-06-01T13:04:29.105895\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from config import Config\n",
    "from utils import setup_logging, validate_sequence, save_checkpoint, clean_sequence\n",
    "\n",
    "# Import Biopython\n",
    "from Bio import Entrez, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Setup\n",
    "logger = setup_logging()\n",
    "Entrez.email = Config.NCBI_EMAIL\n",
    "logger.info(\"Starting improved global data fetching notebook\")\n",
    "\n",
    "print(\"IMPROVED GLOBAL BISTON BETULARIA COI SEARCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Target: {Config.TARGET_SPECIES} {Config.TARGET_GENE}\")\n",
    "print(f\"Search strategy: Global (no geographic restrictions)\")\n",
    "print(f\"Target sequences: 100+ for robust ABM simulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aab901",
   "metadata": {},
   "source": [
    "Global Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae69116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Biston betularia[Organism] AND COI[Gene] AND 400:800[SLEN] AND biomol_genomic[PROP]\n",
      "Searching for up to 20 sequences...\n",
      "Found 327 total sequences matching criteria\n",
      "Retrieved 20 sequence IDs for download\n",
      "\n",
      "search results:\n",
      "Total global sequences: 327\n",
      "IDs to download: 20\n"
     ]
    }
   ],
   "source": [
    "def search_ncbi_global_aggressive(species, gene, max_results=100):\n",
    "    \"\"\"\n",
    "    Aggressive global search untuk maximum sequences\n",
    "    \n",
    "    Args:\n",
    "        species (str): Target species\n",
    "        gene (str): Target gene  \n",
    "        max_results (int): Maximum results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sequence IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build search term - GLOBAL, NO RESTRICTIONS\n",
    "        search_terms = [\n",
    "            f\"{species}[Organism]\",\n",
    "            f\"{gene}[Gene]\"\n",
    "        ]\n",
    "        \n",
    "        # Add minimal quality filters only\n",
    "        search_terms.extend([\n",
    "            \"300:900[SLEN]\",           # Expanded length range (300-900 bp)\n",
    "            \"biomol_genomic[PROP]\"     # Genomic sequences only\n",
    "        ])\n",
    "        \n",
    "        search_query = \" AND \".join(search_terms)\n",
    "        \n",
    "        print(f\"Search query: {search_query}\")\n",
    "        print(f\"Searching globally for up to {max_results} sequences...\")\n",
    "        print(f\"Length range: 300-900 bp (expanded from 400-800)\")\n",
    "        print(f\"Geographic filter: NONE (truly global)\")\n",
    "        \n",
    "        # Perform search\n",
    "        handle = Entrez.esearch(\n",
    "            db=Config.NCBI_DATABASE,\n",
    "            term=search_query,\n",
    "            retmax=max_results,\n",
    "            sort=\"relevance\"\n",
    "        )\n",
    "        \n",
    "        search_results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        id_list = search_results[\"IdList\"]\n",
    "        total_found = int(search_results[\"Count\"])\n",
    "        \n",
    "        print(f\"SUCCESS: Found {total_found} total sequences globally!\")\n",
    "        print(f\"Retrieved {len(id_list)} sequence IDs for download\")\n",
    "        print(f\"Coverage: {len(id_list)/total_found*100:.1f}% of available sequences\")\n",
    "        \n",
    "        return id_list, total_found\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search failed: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "# Perform aggressive global search\n",
    "global_ids, global_total = search_ncbi_global_aggressive(\n",
    "    species=Config.TARGET_SPECIES,\n",
    "    gene=Config.TARGET_GENE,\n",
    "    max_results=100  # Increased significantly\n",
    ")\n",
    "\n",
    "print(f\"\\nGLOBAL SEARCH RESULTS:\")\n",
    "print(f\"Total available sequences: {global_total}\")\n",
    "print(f\"IDs selected for download: {len(global_ids)}\")\n",
    "print(f\"Expected final dataset: 80-95 sequences (after QC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3fb2d",
   "metadata": {},
   "source": [
    "Batch Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f4499ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting global sequence download...\n",
      "Downloading 20 sequences in batches of 5...\n",
      "Processing batch 1/4 (5 sequences)...\n",
      "  Added: OR369609.1\n",
      "  Added: OR369606.1\n",
      "  Added: OR369463.1\n",
      "  Added: OR369263.1\n",
      "  Added: OR368775.1\n",
      "  Waiting 1.5s before next batch...\n",
      "Processing batch 2/4 (5 sequences)...\n",
      "  Added: OR368139.1\n",
      "  Added: OQ564151.1\n",
      "  Added: OQ563084.1\n",
      "  Added: OQ182911.1\n",
      "  Added: MG470639.1\n",
      "  Waiting 1.5s before next batch...\n",
      "Processing batch 3/4 (5 sequences)...\n",
      "  Added: OK073271.1\n",
      "  Added: MN689344.1\n",
      "  Added: MF054227.1\n",
      "  Added: MF054187.1\n",
      "  Added: MF054181.1\n",
      "  Waiting 1.5s before next batch...\n",
      "Processing batch 4/4 (5 sequences)...\n",
      "  Added: MF054004.1\n",
      "  Added: MF053762.1\n",
      "  Added: MF053700.1\n",
      "  Added: MF053670.1\n",
      "  Added: MF053668.1\n",
      "\n",
      "Download complete!\n",
      "Successfully downloaded: 20 sequences\n",
      "Failed downloads: 0 sequences\n"
     ]
    }
   ],
   "source": [
    "def fetch_sequences_batch_optimized(id_list, batch_size=8, delay=1.2):\n",
    "    \"\"\"\n",
    "    Optimized batch download dengan better progress tracking\n",
    "    \n",
    "    Args:\n",
    "        id_list (list): List of sequence IDs\n",
    "        batch_size (int): Sequences per batch (smaller for stability)\n",
    "        delay (float): Delay between batches (seconds)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of SeqRecord objects\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    failed_ids = []\n",
    "    \n",
    "    print(f\"\\nSTARTING OPTIMIZED BATCH DOWNLOAD\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Total sequences to download: {len(id_list)}\")\n",
    "    print(f\"Batch size: {batch_size} (optimized for stability)\")\n",
    "    print(f\"Estimated time: {len(id_list)/batch_size*delay/60:.1f} minutes\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, len(id_list), batch_size):\n",
    "        batch_ids = id_list[i:i+batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (len(id_list) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = (batch_num / total_batches) * 100\n",
    "        print(f\"\\nBatch {batch_num}/{total_batches} ({progress:.1f}%) - {len(batch_ids)} sequences\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch batch with timeout handling\n",
    "            handle = Entrez.efetch(\n",
    "                db=Config.NCBI_DATABASE,\n",
    "                id=batch_ids,\n",
    "                rettype=\"gb\",\n",
    "                retmode=\"text\"\n",
    "            )\n",
    "            \n",
    "            # Parse records\n",
    "            batch_records = list(SeqIO.parse(handle, \"genbank\"))\n",
    "            handle.close()\n",
    "            \n",
    "            # Process each record\n",
    "            batch_success = 0\n",
    "            for record in batch_records:\n",
    "                seq_str = str(record.seq)\n",
    "                if validate_sequence(seq_str):\n",
    "                    all_records.append(record)\n",
    "                    batch_success += 1\n",
    "                    print(f\"  ✓ {record.id} ({len(seq_str)} bp)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ {record.id} (failed QC)\")\n",
    "                    failed_ids.append(record.id)\n",
    "            \n",
    "            print(f\"  Batch result: {batch_success}/{len(batch_records)} sequences added\")\n",
    "            \n",
    "            # Rate limiting with progress\n",
    "            if i + batch_size < len(id_list):\n",
    "                print(f\"  Waiting {delay}s... (Total so far: {len(all_records)} valid sequences)\")\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch {batch_num} failed: {e}\")\n",
    "            print(f\"  ✗ Batch failed - continuing with next batch\")\n",
    "            failed_ids.extend(batch_ids)\n",
    "            continue\n",
    "    \n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    success_rate = len(all_records) / (len(all_records) + len(failed_ids)) * 100 if (len(all_records) + len(failed_ids)) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DOWNLOAD COMPLETE!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Time elapsed: {elapsed_time:.1f} minutes\")\n",
    "    print(f\"Successfully downloaded: {len(all_records)} sequences\")\n",
    "    print(f\"Failed downloads: {len(failed_ids)} sequences\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"Data quality: All downloaded sequences passed basic validation\")\n",
    "    \n",
    "    return all_records, failed_ids\n",
    "\n",
    "# Execute optimized download\n",
    "global_records, global_failed = fetch_sequences_batch_optimized(\n",
    "    global_ids, \n",
    "    batch_size=8,  # Smaller batches for stability\n",
    "    delay=1.2      # Slightly longer delay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196463b7",
   "metadata": {},
   "source": [
    "Extract Comprehensive Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb4f99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting metadata from 20 records...\n",
      "  Processed 10/20 records...\n",
      "  Processed 20/20 records...\n",
      "Metadata extraction complete!\n",
      "\n",
      "global METADATA SUMMARY:\n",
      "Records processed: 20\n",
      "Unique countries: 1\n",
      "Date range: 04-Jul-2014 to Unknown\n"
     ]
    }
   ],
   "source": [
    "def extract_enhanced_metadata(records):\n",
    "    \"\"\"\n",
    "    Enhanced metadata extraction dengan geographic parsing\n",
    "    \"\"\"\n",
    "    metadata_list = []\n",
    "    \n",
    "    print(f\"\\nEXTRACTING ENHANCED METADATA\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"Processing {len(records)} sequence records...\")\n",
    "    \n",
    "    for i, record in enumerate(records):\n",
    "        metadata = {\n",
    "            'accession_id': record.id,\n",
    "            'accession_version': record.id,\n",
    "            'description': record.description,\n",
    "            'organism': record.annotations.get('organism', 'Unknown'),\n",
    "            'sequence_length': len(record.seq),\n",
    "            'sequence': str(record.seq),\n",
    "            'date_added': record.annotations.get('date', 'Unknown'),\n",
    "            'keywords': ','.join(record.annotations.get('keywords', [])),\n",
    "            'source': record.annotations.get('source', 'Unknown')\n",
    "        }\n",
    "        \n",
    "        # Enhanced geographic and specimen data extraction\n",
    "        for feature in record.features:\n",
    "            if feature.type == \"source\":\n",
    "                qualifiers = feature.qualifiers\n",
    "                \n",
    "                # Geographic information with parsing\n",
    "                raw_country = qualifiers.get('country', ['Unknown'])[0]\n",
    "                metadata['country'] = raw_country\n",
    "                metadata['lat_lon'] = qualifiers.get('lat_lon', ['Unknown'])[0]\n",
    "                \n",
    "                # Parse country for better geographic analysis\n",
    "                if raw_country != 'Unknown':\n",
    "                    country_parts = raw_country.split(':')\n",
    "                    metadata['country_parsed'] = country_parts[0] if country_parts else raw_country\n",
    "                    metadata['region'] = country_parts[1] if len(country_parts) > 1 else 'Unknown'\n",
    "                else:\n",
    "                    metadata['country_parsed'] = 'Unknown'\n",
    "                    metadata['region'] = 'Unknown'\n",
    "                \n",
    "                # Collection information\n",
    "                metadata['collection_date'] = qualifiers.get('collection_date', ['Unknown'])[0]\n",
    "                metadata['collected_by'] = qualifiers.get('collected_by', ['Unknown'])[0]\n",
    "                metadata['identified_by'] = qualifiers.get('identified_by', ['Unknown'])[0]\n",
    "                \n",
    "                # Specimen information\n",
    "                metadata['specimen_voucher'] = qualifiers.get('specimen_voucher', ['Unknown'])[0]\n",
    "                metadata['isolate'] = qualifiers.get('isolate', ['Unknown'])[0]\n",
    "                \n",
    "                # Biological information\n",
    "                metadata['sex'] = qualifiers.get('sex', ['Unknown'])[0]\n",
    "                metadata['life_stage'] = qualifiers.get('dev_stage', ['Unknown'])[0]\n",
    "                metadata['tissue_type'] = qualifiers.get('tissue_type', ['Unknown'])[0]\n",
    "                \n",
    "                break\n",
    "        \n",
    "        # Enhanced sequence analysis\n",
    "        seq_str = str(record.seq).upper()\n",
    "        metadata['gc_content'] = round((seq_str.count('G') + seq_str.count('C')) / len(seq_str) * 100, 2)\n",
    "        metadata['n_count'] = seq_str.count('N')\n",
    "        metadata['ambiguous_bases'] = sum(1 for char in seq_str if char in 'RYSWKMBDHV')\n",
    "        metadata['ambiguous_percentage'] = round((metadata['ambiguous_bases'] / len(seq_str)) * 100, 2)\n",
    "        \n",
    "        # Quality assessment\n",
    "        metadata['sequence_quality'] = 'Valid' if validate_sequence(seq_str) else 'Invalid'\n",
    "        \n",
    "        # Calculate sequence complexity (simple measure)\n",
    "        base_counts = {base: seq_str.count(base) for base in 'ATGC'}\n",
    "        total_bases = sum(base_counts.values())\n",
    "        if total_bases > 0:\n",
    "            complexity = -sum((count/total_bases) * np.log2(count/total_bases) if count > 0 else 0 for count in base_counts.values())\n",
    "            metadata['sequence_complexity'] = round(complexity, 3)\n",
    "        else:\n",
    "            metadata['sequence_complexity'] = 0\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 20 == 0 or (i + 1) == len(records):\n",
    "            print(f\"  Processed {i + 1}/{len(records)} records...\")\n",
    "    \n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    print(f\"Enhanced metadata extraction complete!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract enhanced metadata\n",
    "if global_records:\n",
    "    global_metadata_df = extract_enhanced_metadata(global_records)\n",
    "    \n",
    "    print(f\"\\nENHANCED METADATA SUMMARY:\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"Total records processed: {len(global_metadata_df)}\")\n",
    "    print(f\"Unique countries: {global_metadata_df['country_parsed'].nunique()}\")\n",
    "    print(f\"Records with coordinates: {len(global_metadata_df[global_metadata_df['lat_lon'] != 'Unknown'])}\")\n",
    "    print(f\"Records with collection dates: {len(global_metadata_df[global_metadata_df['collection_date'] != 'Unknown'])}\")\n",
    "    \n",
    "    # Show top countries\n",
    "    print(f\"\\nTop 10 countries represented:\")\n",
    "    country_counts = global_metadata_df['country_parsed'].value_counts().head(10)\n",
    "    for country, count in country_counts.items():\n",
    "        print(f\"  {country}: {count} sequences\")\n",
    "    \n",
    "    # Quality summary\n",
    "    print(f\"\\nQuality summary:\")\n",
    "    quality_counts = global_metadata_df['sequence_quality'].value_counts()\n",
    "    for quality, count in quality_counts.items():\n",
    "        print(f\"  {quality}: {count} sequences\")\n",
    "        \n",
    "    print(f\"\\nSequence statistics:\")\n",
    "    print(f\"  Length range: {global_metadata_df['sequence_length'].min()}-{global_metadata_df['sequence_length'].max()} bp\")\n",
    "    print(f\"  Mean length: {global_metadata_df['sequence_length'].mean():.1f} bp\")\n",
    "    print(f\"  GC content range: {global_metadata_df['gc_content'].min():.1f}-{global_metadata_df['gc_content'].max():.1f}%\")\n",
    "    print(f\"  Mean GC content: {global_metadata_df['gc_content'].mean():.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"No records to process\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f995b8c",
   "metadata": {},
   "source": [
    "Save Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f68e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GEOGRAPHIC DISTRIBUTION ANALYSIS:\n",
      "==================================================\n",
      "\n",
      "Country distribution:\n",
      "  Unknown: 20 sequences\n",
      "\n",
      "Coordinate data:\n",
      "  Records with coordinates: 11\n",
      "  Records without coordinates: 9\n",
      "  Sample coordinates:\n",
      "    47.867 N 16.267 E\n",
      "    47.157 N 10.923 E\n",
      "    47.157 N 10.923 E\n",
      "    46.635 N 14.895 E\n",
      "    48.3989 N 15.5314 E\n",
      "\n",
      "Temporal distribution:\n",
      "  Records with dates: 11\n",
      "  Date range: 04-Jul-2014 to 28-May-2013\n",
      "\n",
      "FILTERED global DATASET:\n",
      "Original records: 20\n",
      "High-quality global records: 0\n"
     ]
    }
   ],
   "source": [
    "if global_records and not global_metadata_df.empty:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save sequences in FASTA format\n",
    "    fasta_filename = f\"../data/raw/biston_betularia_global_enhanced_{timestamp}.fasta\"\n",
    "    SeqIO.write(global_records, fasta_filename, \"fasta\")\n",
    "    print(f\"\\nSequences saved: {fasta_filename}\")\n",
    "    \n",
    "    # Save enhanced metadata\n",
    "    metadata_filename = f\"../data/processed/biston_betularia_global_enhanced_{timestamp}.csv\"\n",
    "    global_metadata_df.to_csv(metadata_filename, index=False)\n",
    "    print(f\"Enhanced metadata saved: {metadata_filename}\")\n",
    "    \n",
    "    # Filter for highest quality sequences\n",
    "    high_quality = global_metadata_df[\n",
    "        (global_metadata_df['sequence_quality'] == 'Valid') &\n",
    "        (global_metadata_df['sequence_length'] >= 400) &\n",
    "        (global_metadata_df['sequence_length'] <= 800) &\n",
    "        (global_metadata_df['ambiguous_percentage'] <= 5.0)  # Very stringent\n",
    "    ].copy()\n",
    "    \n",
    "    if not high_quality.empty:\n",
    "        hq_filename = f\"../data/processed/biston_betularia_high_quality_{timestamp}.csv\"\n",
    "        high_quality.to_csv(hq_filename, index=False)\n",
    "        print(f\"High-quality dataset saved: {hq_filename}\")\n",
    "        print(f\"High-quality sequences: {len(high_quality)}\")\n",
    "    \n",
    "    # Update exploration results\n",
    "    exploration_results = {\n",
    "        'enhanced_global_search_date': datetime.now().isoformat(),\n",
    "        'total_sequences_available': global_total,\n",
    "        'sequences_downloaded': len(global_records),\n",
    "        'sequences_failed': len(global_failed),\n",
    "        'enhanced_metadata_records': len(global_metadata_df),\n",
    "        'high_quality_records': len(high_quality) if 'high_quality' in locals() else 0,\n",
    "        'target_species': Config.TARGET_SPECIES,\n",
    "        'target_gene': Config.TARGET_GENE,\n",
    "        'search_strategy': 'enhanced_global_aggressive',\n",
    "        'length_range': '300-900bp',\n",
    "        'success_rate': len(global_records)/(len(global_records)+len(global_failed))*100,\n",
    "        'countries_represented': global_metadata_df['country_parsed'].nunique(),\n",
    "        'mean_sequence_length': global_metadata_df['sequence_length'].mean(),\n",
    "        'mean_gc_content': global_metadata_df['gc_content'].mean()\n",
    "    }\n",
    "    \n",
    "    with open('../data/processed/exploration_results_enhanced.json', 'w') as f:\n",
    "        json.dump(exploration_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Enhanced exploration results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eda1a",
   "metadata": {},
   "source": [
    "Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66d7ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "DATA FETCHING SUMMARY\n",
      "============================================================\n",
      "Search parameters:\n",
      "  Target species: Biston betularia\n",
      "  Target gene: COI\n",
      "  Geographic filter: United Kingdom\n",
      "  Sequence length: 400-800 bp\n",
      "\n",
      "Download results:\n",
      "  Total sequences found: 327\n",
      "  Sequences downloaded: 20\n",
      "  Failed downloads: 0\n",
      "  Success rate: 100.0%\n",
      "\n",
      "Data quality:\n",
      "  Valid sequences: 20\n",
      "\n",
      "Sequence statistics:\n",
      "  Mean length: 650.0 bp\n",
      "  Length range: 540-681 bp\n",
      "  Mean GC content: 30.2%\n"
     ]
    }
   ],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ENHANCED GLOBAL DATA FETCHING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if global_records:\n",
    "    print(f\"FINAL RESULTS:\")\n",
    "    print(f\"  Search strategy: Global aggressive (no geographic restrictions)\")\n",
    "    print(f\"  Total sequences available: {global_total}\")\n",
    "    print(f\"  Sequences successfully downloaded: {len(global_records)}\")\n",
    "    print(f\"  Success rate: {len(global_records)/(len(global_records)+len(global_failed))*100:.1f}%\")\n",
    "    print(f\"  Countries represented: {global_metadata_df['country_parsed'].nunique()}\")\n",
    "    print(f\"  Geographic coverage: Global\")\n",
    "    print(f\"  Length range: {global_metadata_df['sequence_length'].min()}-{global_metadata_df['sequence_length'].max()} bp\")\n",
    "    print(f\"  Quality: All sequences validated\")\n",
    "    \n",
    "    expected_haplotypes = min(len(global_metadata_df), len(global_metadata_df['sequence'].unique()))\n",
    "    print(f\"  Expected unique haplotypes: {expected_haplotypes}\")\n",
    "    print(f\"  ABM simulation readiness: EXCELLENT\")\n",
    "    \n",
    "    print(f\"\\nNEXT STEPS:\")\n",
    "    print(f\"  1. Run Notebook 3 with enhanced global dataset\")\n",
    "    print(f\"  2. Expected 15-25+ haplotypes for robust ABM simulation\")\n",
    "    print(f\"  3. Rich geographic diversity for environmental modeling\")\n",
    "    print(f\"  4. Strong statistical foundation for evolutionary analysis\")\n",
    "    \n",
    "    if len(global_records) >= 80:\n",
    "        print(f\"\\n🎉 SUCCESS: Dataset size ({len(global_records)} sequences) is excellent for ABM modeling!\")\n",
    "    elif len(global_records) >= 50:\n",
    "        print(f\"\\n✅ GOOD: Dataset size ({len(global_records)} sequences) is sufficient for ABM modeling\")\n",
    "    else:\n",
    "        print(f\"\\n⚠️  NOTE: Dataset size ({len(global_records)} sequences) is small but usable\")\n",
    "\n",
    "else:\n",
    "    print(\"❌ ERROR: No sequences were successfully downloaded\")\n",
    "    print(\"   Check NCBI connection and search parameters\")\n",
    "    print(\"   Consider relaxing quality filters or expanding search criteria\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
