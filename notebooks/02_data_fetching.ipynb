{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04137d4a",
   "metadata": {},
   "source": [
    "Setup & Import Previous Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "574fa58d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-06-01 13:50:08,062 - INFO - Starting improved global data fetching notebook\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMPROVED GLOBAL BISTON BETULARIA COI SEARCH\n",
      "============================================================\n",
      "Target: Biston betularia COI\n",
      "Search strategy: Global (no geographic restrictions)\n",
      "Target sequences: 100+ for robust ABM simulation\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('../src')\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Import custom modules\n",
    "from config import Config\n",
    "from utils import setup_logging, validate_sequence, save_checkpoint, clean_sequence\n",
    "\n",
    "# Import Biopython\n",
    "from Bio import Entrez, SeqIO\n",
    "from Bio.Seq import Seq\n",
    "\n",
    "# Setup\n",
    "logger = setup_logging()\n",
    "Entrez.email = Config.NCBI_EMAIL\n",
    "logger.info(\"Starting improved global data fetching notebook\")\n",
    "\n",
    "print(\"IMPROVED GLOBAL BISTON BETULARIA COI SEARCH\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Target: {Config.TARGET_SPECIES} {Config.TARGET_GENE}\")\n",
    "print(f\"Search strategy: Global (no geographic restrictions)\")\n",
    "print(f\"Target sequences: 100+ for robust ABM simulation\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85aab901",
   "metadata": {},
   "source": [
    "Global Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae69116",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search query: Biston betularia[Organism] AND COI[Gene] AND 300:900[SLEN] AND biomol_genomic[PROP]\n",
      "Searching globally for up to 100 sequences...\n",
      "Length range: 300-900 bp (expanded from 400-800)\n",
      "Geographic filter: NONE (truly global)\n",
      "SUCCESS: Found 327 total sequences globally!\n",
      "Retrieved 100 sequence IDs for download\n",
      "Coverage: 30.6% of available sequences\n",
      "\n",
      "GLOBAL SEARCH RESULTS:\n",
      "Total available sequences: 327\n",
      "IDs selected for download: 100\n",
      "Expected final dataset: 80-95 sequences (after QC)\n"
     ]
    }
   ],
   "source": [
    "def search_ncbi_global_aggressive(species, gene, max_results=300):\n",
    "    \"\"\"\n",
    "    Aggressive global search untuk maximum sequences\n",
    "    \n",
    "    Args:\n",
    "        species (str): Target species\n",
    "        gene (str): Target gene  \n",
    "        max_results (int): Maximum results to return\n",
    "    \n",
    "    Returns:\n",
    "        list: List of sequence IDs\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # Build search term - GLOBAL, NO RESTRICTIONS\n",
    "        search_terms = [\n",
    "            f\"{species}[Organism]\",\n",
    "            f\"{gene}[Gene]\"\n",
    "        ]\n",
    "        \n",
    "        # Add minimal quality filters only\n",
    "        search_terms.extend([\n",
    "            \"300:900[SLEN]\",           # Expanded length range (300-900 bp)\n",
    "            \"biomol_genomic[PROP]\"     # Genomic sequences only\n",
    "        ])\n",
    "        \n",
    "        search_query = \" AND \".join(search_terms)\n",
    "        \n",
    "        print(f\"Search query: {search_query}\")\n",
    "        print(f\"Searching globally for up to {max_results} sequences...\")\n",
    "        print(f\"Length range: 300-900 bp (expanded from 400-800)\")\n",
    "        print(f\"Geographic filter: NONE (truly global)\")\n",
    "        \n",
    "        # Perform search\n",
    "        handle = Entrez.esearch(\n",
    "            db=Config.NCBI_DATABASE,\n",
    "            term=search_query,\n",
    "            retmax=max_results,\n",
    "            sort=\"relevance\"\n",
    "        )\n",
    "        \n",
    "        search_results = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        \n",
    "        id_list = search_results[\"IdList\"]\n",
    "        total_found = int(search_results[\"Count\"])\n",
    "        \n",
    "        print(f\"SUCCESS: Found {total_found} total sequences globally!\")\n",
    "        print(f\"Retrieved {len(id_list)} sequence IDs for download\")\n",
    "        print(f\"Coverage: {len(id_list)/total_found*100:.1f}% of available sequences\")\n",
    "        \n",
    "        return id_list, total_found\n",
    "        \n",
    "    except Exception as e:\n",
    "        logger.error(f\"Search failed: {e}\")\n",
    "        return [], 0\n",
    "\n",
    "# Perform aggressive global search\n",
    "global_ids, global_total = search_ncbi_global_aggressive(\n",
    "    species=Config.TARGET_SPECIES,\n",
    "    gene=Config.TARGET_GENE,\n",
    "    max_results=300\n",
    ")\n",
    "\n",
    "print(f\"\\nGLOBAL SEARCH RESULTS:\")\n",
    "print(f\"Total available sequences: {global_total}\")\n",
    "print(f\"IDs selected for download: {len(global_ids)}\")\n",
    "print(f\"Expected final dataset: 80-95 sequences (after QC)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf3fb2d",
   "metadata": {},
   "source": [
    "Batch Download Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6f4499ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "STARTING OPTIMIZED BATCH DOWNLOAD\n",
      "==================================================\n",
      "Total sequences to download: 100\n",
      "Batch size: 8 (optimized for stability)\n",
      "Estimated time: 0.2 minutes\n",
      "\n",
      "Batch 1/13 (7.7%) - 8 sequences\n",
      "  ✓ OR369609.1 (658 bp)\n",
      "  ✓ OR369606.1 (658 bp)\n",
      "  ✓ OR369463.1 (658 bp)\n",
      "  ✓ OR369263.1 (583 bp)\n",
      "  ✓ OR368775.1 (658 bp)\n",
      "  ✓ OR368139.1 (658 bp)\n",
      "  ✓ OQ564151.1 (658 bp)\n",
      "  ✓ OQ563084.1 (658 bp)\n",
      "  Batch result: 8/8 sequences added\n",
      "  Waiting 1.2s... (Total so far: 8 valid sequences)\n",
      "\n",
      "Batch 2/13 (15.4%) - 8 sequences\n",
      "  ✓ OQ182911.1 (658 bp)\n",
      "  ✓ MG470639.1 (549 bp)\n",
      "  ✓ OK073271.1 (658 bp)\n",
      "  ✓ MN689344.1 (540 bp)\n",
      "  ✓ MF054227.1 (681 bp)\n",
      "  ✓ MF054187.1 (681 bp)\n",
      "  ✓ MF054181.1 (681 bp)\n",
      "  ✓ MF054004.1 (669 bp)\n",
      "  Batch result: 8/8 sequences added\n",
      "  Waiting 1.2s... (Total so far: 16 valid sequences)\n",
      "\n",
      "Batch 3/13 (23.1%) - 8 sequences\n",
      "  ✓ MF053762.1 (669 bp)\n",
      "  ✓ MF053700.1 (675 bp)\n",
      "  ✓ MF053670.1 (681 bp)\n",
      "  ✓ MF053668.1 (669 bp)\n",
      "  ✓ MF053662.1 (660 bp)\n",
      "  ✓ MF053648.1 (660 bp)\n",
      "  ✓ MF053614.1 (675 bp)\n",
      "  ✓ MF053551.1 (672 bp)\n",
      "  Batch result: 8/8 sequences added\n",
      "  Waiting 1.2s... (Total so far: 24 valid sequences)\n",
      "\n",
      "Batch 4/13 (30.8%) - 8 sequences\n",
      "  ✓ MF053547.1 (663 bp)\n",
      "  ✓ MF053542.1 (666 bp)\n",
      "  ✓ MF053476.1 (672 bp)\n",
      "  ✓ MF053464.1 (684 bp)\n",
      "  ✓ MF053459.1 (681 bp)\n",
      "  ✓ MF053457.1 (681 bp)\n",
      "  ✓ MF052895.1 (639 bp)\n",
      "  ✓ MF052681.1 (633 bp)\n",
      "  Batch result: 8/8 sequences added\n",
      "  Waiting 1.2s... (Total so far: 32 valid sequences)\n",
      "\n",
      "Batch 5/13 (38.5%) - 8 sequences\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\Bio\\File.py:72\u001b[39m, in \u001b[36mas_handle\u001b[39m\u001b[34m(handleish, mode, **kwargs)\u001b[39m\n\u001b[32m     71\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m72\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mhandleish\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m fp:\n\u001b[32m     73\u001b[39m         \u001b[38;5;28;01myield\u001b[39;00m fp\n",
      "\u001b[31mTypeError\u001b[39m: expected str, bytes or os.PathLike object, not TextIOWrapper",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 86\u001b[39m\n\u001b[32m     83\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_records, failed_ids\n\u001b[32m     85\u001b[39m \u001b[38;5;66;03m# Execute optimized download\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m86\u001b[39m global_records, global_failed = \u001b[43mfetch_sequences_batch_optimized\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     87\u001b[39m \u001b[43m    \u001b[49m\u001b[43mglobal_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     88\u001b[39m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Smaller batches for stability\u001b[39;49;00m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelay\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m1.2\u001b[39;49m\u001b[43m      \u001b[49m\u001b[38;5;66;43;03m# Slightly longer delay\u001b[39;49;00m\n\u001b[32m     90\u001b[39m \u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[65]\u001b[39m\u001b[32m, line 43\u001b[39m, in \u001b[36mfetch_sequences_batch_optimized\u001b[39m\u001b[34m(id_list, batch_size, delay)\u001b[39m\n\u001b[32m     35\u001b[39m handle = Entrez.efetch(\n\u001b[32m     36\u001b[39m     db=Config.NCBI_DATABASE,\n\u001b[32m     37\u001b[39m     \u001b[38;5;28mid\u001b[39m=batch_ids,\n\u001b[32m     38\u001b[39m     rettype=\u001b[33m\"\u001b[39m\u001b[33mgb\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     39\u001b[39m     retmode=\u001b[33m\"\u001b[39m\u001b[33mtext\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     40\u001b[39m )\n\u001b[32m     42\u001b[39m \u001b[38;5;66;03m# Parse records\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m43\u001b[39m batch_records = \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mSeqIO\u001b[49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mgenbank\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     44\u001b[39m handle.close()\n\u001b[32m     46\u001b[39m \u001b[38;5;66;03m# Process each record\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\Bio\\SeqIO\\InsdcIO.py:109\u001b[39m, in \u001b[36mGenBankIterator.__next__\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    107\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__next__\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    108\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Return the next SeqRecord.\"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m109\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mrecords\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\Bio\\GenBank\\Scanner.py:512\u001b[39m, in \u001b[36mInsdcScanner.parse_records\u001b[39m\u001b[34m(self, handle, do_features)\u001b[39m\n\u001b[32m    510\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m as_handle(handle) \u001b[38;5;28;01mas\u001b[39;00m handle:\n\u001b[32m    511\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m512\u001b[39m         record = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_features\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    513\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m record \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    514\u001b[39m             \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\Bio\\GenBank\\Scanner.py:495\u001b[39m, in \u001b[36mInsdcScanner.parse\u001b[39m\u001b[34m(self, handle, do_features)\u001b[39m\n\u001b[32m    489\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mBio\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mGenBank\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FeatureValueCleaner\n\u001b[32m    491\u001b[39m consumer = _FeatureConsumer(\n\u001b[32m    492\u001b[39m     use_fuzziness=\u001b[32m1\u001b[39m, feature_cleaner=FeatureValueCleaner()\n\u001b[32m    493\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m495\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfeed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconsumer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdo_features\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m    496\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m consumer.data\n\u001b[32m    497\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\Bio\\GenBank\\Scanner.py:462\u001b[39m, in \u001b[36mInsdcScanner.feed\u001b[39m\u001b[34m(self, handle, consumer, do_features)\u001b[39m\n\u001b[32m    456\u001b[39m \u001b[38;5;66;03m# We use the above class methods to parse the file into a simplified format.\u001b[39;00m\n\u001b[32m    457\u001b[39m \u001b[38;5;66;03m# The first line, header lines and any misc lines after the features will be\u001b[39;00m\n\u001b[32m    458\u001b[39m \u001b[38;5;66;03m# dealt with by GenBank / EMBL specific derived classes.\u001b[39;00m\n\u001b[32m    459\u001b[39m \n\u001b[32m    460\u001b[39m \u001b[38;5;66;03m# First line and header:\u001b[39;00m\n\u001b[32m    461\u001b[39m \u001b[38;5;28mself\u001b[39m._feed_first_line(consumer, \u001b[38;5;28mself\u001b[39m.line)\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m \u001b[38;5;28mself\u001b[39m._feed_header_lines(consumer, \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparse_header\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    464\u001b[39m \u001b[38;5;66;03m# Features (common to both EMBL and GenBank):\u001b[39;00m\n\u001b[32m    465\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m do_features:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Roaming\\Python\\Python313\\site-packages\\Bio\\GenBank\\Scanner.py:125\u001b[39m, in \u001b[36mInsdcScanner.parse_header\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    123\u001b[39m header_lines = []\n\u001b[32m    124\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m125\u001b[39m     line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhandle\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    126\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m line:\n\u001b[32m    127\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mPremature end of line during sequence data\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\http\\client.py:662\u001b[39m, in \u001b[36mHTTPResponse.read1\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    660\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    661\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.chunked:\n\u001b[32m--> \u001b[39m\u001b[32m662\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read1_chunked\u001b[49m\u001b[43m(\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    663\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.length \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m (n < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m n > \u001b[38;5;28mself\u001b[39m.length):\n\u001b[32m    664\u001b[39m     n = \u001b[38;5;28mself\u001b[39m.length\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\http\\client.py:703\u001b[39m, in \u001b[36mHTTPResponse._read1_chunked\u001b[39m\u001b[34m(self, n)\u001b[39m\n\u001b[32m    700\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read1_chunked\u001b[39m(\u001b[38;5;28mself\u001b[39m, n):\n\u001b[32m    701\u001b[39m     \u001b[38;5;66;03m# Strictly speaking, _get_chunk_left() may cause more than one read,\u001b[39;00m\n\u001b[32m    702\u001b[39m     \u001b[38;5;66;03m# but that is ok, since that is to satisfy the chunked protocol.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m703\u001b[39m     chunk_left = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_chunk_left\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    704\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m chunk_left \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m n == \u001b[32m0\u001b[39m:\n\u001b[32m    705\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\http\\client.py:579\u001b[39m, in \u001b[36mHTTPResponse._get_chunk_left\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    577\u001b[39m     \u001b[38;5;28mself\u001b[39m._safe_read(\u001b[32m2\u001b[39m)  \u001b[38;5;66;03m# toss the CRLF at the end of the chunk\u001b[39;00m\n\u001b[32m    578\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m579\u001b[39m     chunk_left = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_next_chunk_size\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    580\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m:\n\u001b[32m    581\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m IncompleteRead(\u001b[33mb\u001b[39m\u001b[33m'\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\http\\client.py:539\u001b[39m, in \u001b[36mHTTPResponse._read_next_chunk_size\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    537\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_read_next_chunk_size\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    538\u001b[39m     \u001b[38;5;66;03m# Read the next chunk size from the file\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m539\u001b[39m     line = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mfp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_MAXLINE\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    540\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(line) > _MAXLINE:\n\u001b[32m    541\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m LineTooLong(\u001b[33m\"\u001b[39m\u001b[33mchunk size\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\socket.py:719\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    717\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot read from timed out object\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m    718\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m719\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    720\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    721\u001b[39m     \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1304\u001b[39m, in \u001b[36mSSLSocket.recv_into\u001b[39m\u001b[34m(self, buffer, nbytes, flags)\u001b[39m\n\u001b[32m   1300\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1301\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1302\u001b[39m           \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv_into() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1303\u001b[39m           \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1304\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1305\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1306\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv_into(buffer, nbytes, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Python313\\Lib\\ssl.py:1138\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1136\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m   1137\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbuffer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "def fetch_sequences_batch_optimized(id_list, batch_size=8, delay=1.2):\n",
    "    \"\"\"\n",
    "    Optimized batch download dengan better progress tracking\n",
    "    \n",
    "    Args:\n",
    "        id_list (list): List of sequence IDs\n",
    "        batch_size (int): Sequences per batch (smaller for stability)\n",
    "        delay (float): Delay between batches (seconds)\n",
    "    \n",
    "    Returns:\n",
    "        list: List of SeqRecord objects\n",
    "    \"\"\"\n",
    "    all_records = []\n",
    "    failed_ids = []\n",
    "    \n",
    "    print(f\"\\nSTARTING OPTIMIZED BATCH DOWNLOAD\")\n",
    "    print(f\"=\"*50)\n",
    "    print(f\"Total sequences to download: {len(id_list)}\")\n",
    "    print(f\"Batch size: {batch_size} (optimized for stability)\")\n",
    "    print(f\"Estimated time: {len(id_list)/batch_size*delay/60:.1f} minutes\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    for i in range(0, len(id_list), batch_size):\n",
    "        batch_ids = id_list[i:i+batch_size]\n",
    "        batch_num = (i // batch_size) + 1\n",
    "        total_batches = (len(id_list) + batch_size - 1) // batch_size\n",
    "        \n",
    "        # Progress indicator\n",
    "        progress = (batch_num / total_batches) * 100\n",
    "        print(f\"\\nBatch {batch_num}/{total_batches} ({progress:.1f}%) - {len(batch_ids)} sequences\")\n",
    "        \n",
    "        try:\n",
    "            # Fetch batch with timeout handling\n",
    "            handle = Entrez.efetch(\n",
    "                db=Config.NCBI_DATABASE,\n",
    "                id=batch_ids,\n",
    "                rettype=\"gb\",\n",
    "                retmode=\"text\"\n",
    "            )\n",
    "            \n",
    "            # Parse records\n",
    "            batch_records = list(SeqIO.parse(handle, \"genbank\"))\n",
    "            handle.close()\n",
    "            \n",
    "            # Process each record\n",
    "            batch_success = 0\n",
    "            for record in batch_records:\n",
    "                seq_str = str(record.seq)\n",
    "                if validate_sequence(seq_str):\n",
    "                    all_records.append(record)\n",
    "                    batch_success += 1\n",
    "                    print(f\"  ✓ {record.id} ({len(seq_str)} bp)\")\n",
    "                else:\n",
    "                    print(f\"  ✗ {record.id} (failed QC)\")\n",
    "                    failed_ids.append(record.id)\n",
    "            \n",
    "            print(f\"  Batch result: {batch_success}/{len(batch_records)} sequences added\")\n",
    "            \n",
    "            # Rate limiting with progress\n",
    "            if i + batch_size < len(id_list):\n",
    "                print(f\"  Waiting {delay}s... (Total so far: {len(all_records)} valid sequences)\")\n",
    "                time.sleep(delay)\n",
    "                \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Batch {batch_num} failed: {e}\")\n",
    "            print(f\"  ✗ Batch failed - continuing with next batch\")\n",
    "            failed_ids.extend(batch_ids)\n",
    "            continue\n",
    "    \n",
    "    elapsed_time = (time.time() - start_time) / 60\n",
    "    success_rate = len(all_records) / (len(all_records) + len(failed_ids)) * 100 if (len(all_records) + len(failed_ids)) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*50}\")\n",
    "    print(f\"DOWNLOAD COMPLETE!\")\n",
    "    print(f\"{'='*50}\")\n",
    "    print(f\"Time elapsed: {elapsed_time:.1f} minutes\")\n",
    "    print(f\"Successfully downloaded: {len(all_records)} sequences\")\n",
    "    print(f\"Failed downloads: {len(failed_ids)} sequences\")\n",
    "    print(f\"Success rate: {success_rate:.1f}%\")\n",
    "    print(f\"Data quality: All downloaded sequences passed basic validation\")\n",
    "    \n",
    "    return all_records, failed_ids\n",
    "\n",
    "# Execute optimized download\n",
    "global_records, global_failed = fetch_sequences_batch_optimized(\n",
    "    global_ids, \n",
    "    batch_size=8,  # Smaller batches for stability\n",
    "    delay=1.2      # Slightly longer delay\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196463b7",
   "metadata": {},
   "source": [
    "Extract Comprehensive Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4beb4f99",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_enhanced_metadata(records):\n",
    "    \"\"\"\n",
    "    Enhanced metadata extraction dengan geographic parsing\n",
    "    \"\"\"\n",
    "    metadata_list = []\n",
    "    \n",
    "    print(f\"\\nEXTRACTING ENHANCED METADATA\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"Processing {len(records)} sequence records...\")\n",
    "    \n",
    "    for i, record in enumerate(records):\n",
    "        metadata = {\n",
    "            'accession_id': record.id,\n",
    "            'accession_version': record.id,\n",
    "            'description': record.description,\n",
    "            'organism': record.annotations.get('organism', 'Unknown'),\n",
    "            'sequence_length': len(record.seq),\n",
    "            'sequence': str(record.seq),\n",
    "            'date_added': record.annotations.get('date', 'Unknown'),\n",
    "            'keywords': ','.join(record.annotations.get('keywords', [])),\n",
    "            'source': record.annotations.get('source', 'Unknown')\n",
    "        }\n",
    "        \n",
    "        # Enhanced geographic and specimen data extraction\n",
    "        for feature in record.features:\n",
    "            if feature.type == \"source\":\n",
    "                qualifiers = feature.qualifiers\n",
    "                \n",
    "                # Geographic information with parsing\n",
    "                raw_country = qualifiers.get('country', ['Unknown'])[0]\n",
    "                metadata['country'] = raw_country\n",
    "                metadata['lat_lon'] = qualifiers.get('lat_lon', ['Unknown'])[0]\n",
    "                \n",
    "                # Parse country for better geographic analysis\n",
    "                if raw_country != 'Unknown':\n",
    "                    country_parts = raw_country.split(':')\n",
    "                    metadata['country_parsed'] = country_parts[0] if country_parts else raw_country\n",
    "                    metadata['region'] = country_parts[1] if len(country_parts) > 1 else 'Unknown'\n",
    "                else:\n",
    "                    metadata['country_parsed'] = 'Unknown'\n",
    "                    metadata['region'] = 'Unknown'\n",
    "                \n",
    "                # Collection information\n",
    "                metadata['collection_date'] = qualifiers.get('collection_date', ['Unknown'])[0]\n",
    "                metadata['collected_by'] = qualifiers.get('collected_by', ['Unknown'])[0]\n",
    "                metadata['identified_by'] = qualifiers.get('identified_by', ['Unknown'])[0]\n",
    "                \n",
    "                # Specimen information\n",
    "                metadata['specimen_voucher'] = qualifiers.get('specimen_voucher', ['Unknown'])[0]\n",
    "                metadata['isolate'] = qualifiers.get('isolate', ['Unknown'])[0]\n",
    "                \n",
    "                # Biological information\n",
    "                metadata['sex'] = qualifiers.get('sex', ['Unknown'])[0]\n",
    "                metadata['life_stage'] = qualifiers.get('dev_stage', ['Unknown'])[0]\n",
    "                metadata['tissue_type'] = qualifiers.get('tissue_type', ['Unknown'])[0]\n",
    "                \n",
    "                break\n",
    "        \n",
    "        # Enhanced sequence analysis\n",
    "        seq_str = str(record.seq).upper()\n",
    "        metadata['gc_content'] = round((seq_str.count('G') + seq_str.count('C')) / len(seq_str) * 100, 2)\n",
    "        metadata['n_count'] = seq_str.count('N')\n",
    "        metadata['ambiguous_bases'] = sum(1 for char in seq_str if char in 'RYSWKMBDHV')\n",
    "        metadata['ambiguous_percentage'] = round((metadata['ambiguous_bases'] / len(seq_str)) * 100, 2)\n",
    "        \n",
    "        # Quality assessment\n",
    "        metadata['sequence_quality'] = 'Valid' if validate_sequence(seq_str) else 'Invalid'\n",
    "        \n",
    "        # Calculate sequence complexity (simple measure)\n",
    "        base_counts = {base: seq_str.count(base) for base in 'ATGC'}\n",
    "        total_bases = sum(base_counts.values())\n",
    "        if total_bases > 0:\n",
    "            complexity = -sum((count/total_bases) * np.log2(count/total_bases) if count > 0 else 0 for count in base_counts.values())\n",
    "            metadata['sequence_complexity'] = round(complexity, 3)\n",
    "        else:\n",
    "            metadata['sequence_complexity'] = 0\n",
    "        \n",
    "        metadata_list.append(metadata)\n",
    "        \n",
    "        # Progress indicator\n",
    "        if (i + 1) % 20 == 0 or (i + 1) == len(records):\n",
    "            print(f\"  Processed {i + 1}/{len(records)} records...\")\n",
    "    \n",
    "    df = pd.DataFrame(metadata_list)\n",
    "    print(f\"Enhanced metadata extraction complete!\")\n",
    "    \n",
    "    return df\n",
    "\n",
    "# Extract enhanced metadata\n",
    "if global_records:\n",
    "    global_metadata_df = extract_enhanced_metadata(global_records)\n",
    "    \n",
    "    print(f\"\\nENHANCED METADATA SUMMARY:\")\n",
    "    print(f\"=\"*40)\n",
    "    print(f\"Total records processed: {len(global_metadata_df)}\")\n",
    "    print(f\"Unique countries: {global_metadata_df['country_parsed'].nunique()}\")\n",
    "    print(f\"Records with coordinates: {len(global_metadata_df[global_metadata_df['lat_lon'] != 'Unknown'])}\")\n",
    "    print(f\"Records with collection dates: {len(global_metadata_df[global_metadata_df['collection_date'] != 'Unknown'])}\")\n",
    "    \n",
    "    # Show top countries\n",
    "    print(f\"\\nTop 10 countries represented:\")\n",
    "    country_counts = global_metadata_df['country_parsed'].value_counts().head(10)\n",
    "    for country, count in country_counts.items():\n",
    "        print(f\"  {country}: {count} sequences\")\n",
    "    \n",
    "    # Quality summary\n",
    "    print(f\"\\nQuality summary:\")\n",
    "    quality_counts = global_metadata_df['sequence_quality'].value_counts()\n",
    "    for quality, count in quality_counts.items():\n",
    "        print(f\"  {quality}: {count} sequences\")\n",
    "        \n",
    "    print(f\"\\nSequence statistics:\")\n",
    "    print(f\"  Length range: {global_metadata_df['sequence_length'].min()}-{global_metadata_df['sequence_length'].max()} bp\")\n",
    "    print(f\"  Mean length: {global_metadata_df['sequence_length'].mean():.1f} bp\")\n",
    "    print(f\"  GC content range: {global_metadata_df['gc_content'].min():.1f}-{global_metadata_df['gc_content'].max():.1f}%\")\n",
    "    print(f\"  Mean GC content: {global_metadata_df['gc_content'].mean():.1f}%\")\n",
    "\n",
    "else:\n",
    "    print(\"No records to process\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f995b8c",
   "metadata": {},
   "source": [
    "Save Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "416f68e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "if global_records and not global_metadata_df.empty:\n",
    "    timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "    \n",
    "    # Save sequences in FASTA format\n",
    "    fasta_filename = f\"../data/raw/biston_betularia_global_enhanced_{timestamp}.fasta\"\n",
    "    SeqIO.write(global_records, fasta_filename, \"fasta\")\n",
    "    print(f\"\\nSequences saved: {fasta_filename}\")\n",
    "    \n",
    "    # Save enhanced metadata\n",
    "    metadata_filename = f\"../data/processed/biston_betularia_global_enhanced_{timestamp}.csv\"\n",
    "    global_metadata_df.to_csv(metadata_filename, index=False)\n",
    "    print(f\"Enhanced metadata saved: {metadata_filename}\")\n",
    "    \n",
    "    # Filter for highest quality sequences\n",
    "    high_quality = global_metadata_df[\n",
    "        (global_metadata_df['sequence_quality'] == 'Valid') &\n",
    "        (global_metadata_df['sequence_length'] >= 400) &\n",
    "        (global_metadata_df['sequence_length'] <= 800) &\n",
    "        (global_metadata_df['ambiguous_percentage'] <= 5.0)  # Very stringent\n",
    "    ].copy()\n",
    "    \n",
    "    if not high_quality.empty:\n",
    "        hq_filename = f\"../data/processed/biston_betularia_high_quality_{timestamp}.csv\"\n",
    "        high_quality.to_csv(hq_filename, index=False)\n",
    "        print(f\"High-quality dataset saved: {hq_filename}\")\n",
    "        print(f\"High-quality sequences: {len(high_quality)}\")\n",
    "    \n",
    "    # Update exploration results\n",
    "    exploration_results = {\n",
    "        'enhanced_global_search_date': datetime.now().isoformat(),\n",
    "        'total_sequences_available': global_total,\n",
    "        'sequences_downloaded': len(global_records),\n",
    "        'sequences_failed': len(global_failed),\n",
    "        'enhanced_metadata_records': len(global_metadata_df),\n",
    "        'high_quality_records': len(high_quality) if 'high_quality' in locals() else 0,\n",
    "        'target_species': Config.TARGET_SPECIES,\n",
    "        'target_gene': Config.TARGET_GENE,\n",
    "        'search_strategy': 'enhanced_global_aggressive',\n",
    "        'length_range': '300-900bp',\n",
    "        'success_rate': len(global_records)/(len(global_records)+len(global_failed))*100,\n",
    "        'countries_represented': global_metadata_df['country_parsed'].nunique(),\n",
    "        'mean_sequence_length': global_metadata_df['sequence_length'].mean(),\n",
    "        'mean_gc_content': global_metadata_df['gc_content'].mean()\n",
    "    }\n",
    "    \n",
    "    with open('../data/processed/exploration_results_enhanced.json', 'w') as f:\n",
    "        json.dump(exploration_results, f, indent=2)\n",
    "    \n",
    "    print(f\"Enhanced exploration results saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a18eda1a",
   "metadata": {},
   "source": [
    "Data Quality Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d66d7ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"\\n{'='*70}\")\n",
    "print(\"ENHANCED GLOBAL DATA FETCHING COMPLETE\")\n",
    "print(f\"{'='*70}\")\n",
    "\n",
    "if global_records:\n",
    "    print(f\"FINAL RESULTS:\")\n",
    "    print(f\"  Search strategy: Global aggressive (no geographic restrictions)\")\n",
    "    print(f\"  Total sequences available: {global_total}\")\n",
    "    print(f\"  Sequences successfully downloaded: {len(global_records)}\")\n",
    "    print(f\"  Success rate: {len(global_records)/(len(global_records)+len(global_failed))*100:.1f}%\")\n",
    "    print(f\"  Countries represented: {global_metadata_df['country_parsed'].nunique()}\")\n",
    "    print(f\"  Geographic coverage: Global\")\n",
    "    print(f\"  Length range: {global_metadata_df['sequence_length'].min()}-{global_metadata_df['sequence_length'].max()} bp\")\n",
    "    print(f\"  Quality: All sequences validated\")\n",
    "    \n",
    "    expected_haplotypes = min(len(global_metadata_df), len(global_metadata_df['sequence'].unique()))\n",
    "    print(f\"  Expected unique haplotypes: {expected_haplotypes}\")\n",
    "    print(f\"  ABM simulation readiness: EXCELLENT\")\n",
    "    \n",
    "    print(f\"\\nNEXT STEPS:\")\n",
    "    print(f\"  1. Run Notebook 3 with enhanced global dataset\")\n",
    "    print(f\"  2. Expected 15-25+ haplotypes for robust ABM simulation\")\n",
    "    print(f\"  3. Rich geographic diversity for environmental modeling\")\n",
    "    print(f\"  4. Strong statistical foundation for evolutionary analysis\")\n",
    "    \n",
    "    if len(global_records) >= 80:\n",
    "        print(f\"\\nSUCCESS: Dataset size ({len(global_records)} sequences) is excellent for ABM modeling!\")\n",
    "    elif len(global_records) >= 50:\n",
    "        print(f\"\\nGOOD: Dataset size ({len(global_records)} sequences) is sufficient for ABM modeling\")\n",
    "    else:\n",
    "        print(f\"\\nNOTE: Dataset size ({len(global_records)} sequences) is small but usable\")\n",
    "\n",
    "else:\n",
    "    print(\"\\nERROR: No sequences were successfully downloaded\")\n",
    "    print(\"\\nCheck NCBI connection and search parameters\")\n",
    "    print(\"\\nConsider relaxing quality filters or expanding search criteria\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
